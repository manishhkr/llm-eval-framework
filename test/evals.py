from framework.evals_main import evaluate_single, evaluate_models, cleanup_all
from framework.evals import evaluate
from framework.openeval import evaluate_1, retrieve_results, get_client
from framework.metrics import Geval
from framework.metrics import Runner
import json
from framework.metrics import MCPServer, MCPToolCall, LLMTestCase, MCPUseMetric, EvaluationRunner
from framework.utils import FileUpload, Dataset


def main():
    print("\n=== Running single model evaluation ===")
    single_result = evaluate_single()

    # --- Clean Single Evaluation Output ---
    print("\n=== Single Evaluation Result ===")
    print(f"Eval ID : {single_result['eval_id']}")
    print(f"Run ID  : {single_result['run_id']}")
    print(f"Status  : {single_result['results']['status']}")

    c = single_result["results"]["counts"]
    print(f"Passed  : {c.passed}")
    print(f"Failed  : {c.failed}")
    print(f"Total   : {c.total}")

    # --- Multi Model Evaluation ---
    print("\n=== Running multiple model evaluations ===")
    models = ["gpt-4o-mini", "gpt-4.1", "gpt-4o"]
    multi_results = evaluate_models(models)

    # --- Summary Table ---
    print("\n=== Summary Table ===")
    print("Model\tPassed\tFailed\tTotal\tAccuracy")

    for model, result in multi_results.items():
        rc = result["results"]["counts"]
        total = rc.total
        passed = rc.passed
        failed = rc.failed
        acc = round((passed / total) * 100, 1) if total > 0 else 0

        print(f"{model}\t{passed}\t{failed}\t{total}\t{acc}%")

    # --- Cleanup ---
    print("\nStarting cleanup...\n")
    cleanup_all()

    
def idk():

    eval_id, run_id = evaluate_1()
    client = get_client()
    retrieve_results(client, eval_id, run_id)


def okay():

    uploader = FileUpload()
    file_id = uploader.filepath("test/ticket.jsonl")
    created_on=uploader.get_created_datetime(file_id)
    filename=uploader.filename(file_id)
    status=uploader.status(file_id)

    dataset = Dataset()
    print(dataset.validate(file_id,["item.ticket_text", "item.correct_label"]))

    print(file_id)
    print(created_on)
    print(filename)
    print(status)

def GEmetrics():
    # Create a correctness evaluation metric using Geval
    correctness_metric = Geval(
        name="Correctness",
        criteria="Check factual correctness.",
        evaluation_params=["input", "actual_output", "expected_output"],
        judge_model="gpt-4o"
    )

    # Define a sample test case
    test_case = {
        "input": "The dog chased the cat up the tree, who ran up the tree?",
        "actual_output": "It depends, some might consider the cat, while others might argue the dog.",
        "expected_output": "The cat."
    }

    # Initialize the runner with test cases and metrics
    runner = Runner(
        test_cases=[test_case],
        metrics=[correctness_metric]
    )
    print("-------------------------------------------------------------------------------")
    print("\n This is the evaluation output generated by the Geval metric.\n")
    # Run the evaluation
    results = runner.run()

    # Print results
    print(results)
    print("Judge model:", correctness_metric.judge_model)

def MCP_metrics():
    # Define an MCP Server with its tools
    server = MCPServer(
        name="http://localhost:800/mcp",
        transport="streamable-http",
        tools=["add", "subtract"],   # Tools the server exposes
        resources=[],
        prompts=[]
    )

    # Expected tool call for "add"
    tool_call = MCPToolCall(
        name="add",
        args={"a": 5, "b": 7},
        result={"result": 12}
    )

    # Expected tool call for "subtract"
    tool_call_2 = MCPToolCall(
        name="subtract",
        args={"a": 20, "b": 4},
        result={"result": 16}
    )

    # Define LLM Test Case
    test_case = LLMTestCase(
        input="What is 20 - 4?",
        actual_output="The result is 16.",
        mcp_servers=[server],
        mcp_tools_called=[tool_call],  # Tools your model actually used
        mcp_resources_called=[],
        mcp_prompts_called=[]
    )

    # Initialize the MCP-Use metric
    metric = MCPUseMetric(judge_model="gpt-4o-mini")

    # Run evaluation
    runner = EvaluationRunner(
        test_cases=[test_case.to_dict()],
        metrics=[metric]
    )

    results = runner.run()

    # Print results
    print("-------------------------------------------------------------------------------")
    print("\n This is the evaluation output generated by the MCP-Use metric.\n")
    print(results)
    print("Judge model:", metric.judge_model)

def mcp_use_fileupload():
    # --- File Upload Section ---
    uploader = FileUpload()

    # Upload the JSONL file and get its file_id
    file_id = uploader.filepath("test/mcp_testcases.jsonl")

    # Fetch metadata about the uploaded file
    created_on = uploader.get_created_datetime(file_id)
    filename = uploader.filename(file_id)
    status = uploader.status(file_id)

    # Display file upload details
    print("-------------------------------------------------------------------------------")
    print("\n This is the evaluation output generated by the MCP-Use Metric (using uploaded test cases).\n")
    print("File ID:", file_id)
    print("Uploaded:", created_on)
    print("Filename:", filename)
    print("Status:", status)
    print("-" * 50)
    dataset = Dataset()

    # Load test cases (returns list of dicts)
    test_cases = dataset.load(file_id)
    metric = MCPUseMetric(judge_model="gpt-4o-mini")

    # Evaluation runner takes list of cases + metrics
    runner = EvaluationRunner(
        test_cases=test_cases,
        metrics=[metric]
    )

    # Execute evaluation
    results = runner.run()

    # print results
    print(json.dumps(results, indent=2))


if __name__ == "__main__":

    main()
    okay()
    evaluate()
    idk()
    GEmetrics()
    MCP_metrics()
    mcp_use_fileupload()
